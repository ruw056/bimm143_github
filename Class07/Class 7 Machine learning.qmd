---
title: "Class 7: Machine Learning 1"
author: "Eric Wang (PID: A17678188)"
format: pdf
---

## Background
Today we will begin our exploration of some important machine learning methods, namely **clusterring** and **dimensionality reduction**. 

Let's make up some input data for clustering where we know what the natural "clusters" are. 

The function `rnorm()` can be useful here. 

```{r}
hist(rnorm(5000, 300, 5))
```

> Q. Generate 30 random numbers centered at + 3 and -3

```{r}

tmp <- c(rnorm(30, 3), rnorm(30,-3))

x <- cbind(x= tmp, y=rev(tmp))

plot(x)
```


## K-means clustering 

The main function in "base R" for K-means clustering is called `kmeans()` :

```{r}
km <- kmeans(x, 2)
km
```

> Q. What component of the results object details the cluster sizes? 

```{r}
km$size
```

> Q. What component of the results object details the cluster centers?

```{r}
km$centers
```

> Q. What component of the results object details the cluster membership vector (i.e. our main result of which points lie in which cluster)

```{r}
km$cluster
```


> Q. Plot our clustering results with points colored by cluster and also add the cluster centers as new points colored blue? 

```{r}
plot(x,col=km$cluster)
points(km$centers, col="blue",pch=15)

```


> Q. Run `kmeans()` again and this time produce 4 clusters and call your result object `k4`

```{r}
k4 <- kmeans(x, 4)
plot(x, col= k4$cluster)
points(k4$centers, col="blue", pch=15)
```

The metric
```{r}
k4$tot.withinss
```

> Q. Let's try different number of K(centers) from 1 to 30 and see what the best result is? 

```{r}
ans <- NULL
for (i in 1:30){
ans <- c(ans, kmeans(x,centers = i)$tot.withinss)
}
ans
```

```{r}
plot(ans,typ= "o")
```

`tot.withnss` shows how tight the cluster it is. The lower the value the tighter the clusters group. 


**Key-Point:** K-means will impose a clustering structure on your data even if it is not there - it will always give you the answer you asked for even if that answer is silly!

## Hierarchical Clustering

The main function for Hierarchical Clustering is called `hclust()`.
Unlike `kmeans()` (which does all the work for you) you can't just pass `hclust()` our raw input data. It needs a "distance matrix" like the one returned from the `dist()` function. 

```{r}
d <- dist(x)
hc <- hclust(d)
plot(hc)
```




To extract our cluster membership vector from a `hclust()` result object we have to "cut" our tree at a given height to yield separate "groups"/"branches". 

To do this we use the `cutree()` function on our `hclust()` objection: 

```{r}
grps <- cutree(hc,h=8)
grps
```
```{r}
table(grps, km$cluster)
```


## PCA of UK food data

Import the data set of food consumption in the UK 

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
x
```

> Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
dim(x)
```
One soltion to set the row names is by hand...

```{r}
#rownames(x)
rownames(x) <- x[,1]
```

To remove the first column I can use the minus index trick 

```{r}
x <- x[,-1]
x
```

A better way to do this is to set the row names to the first collumn with `read.csv()`

```{r}
x <- read.csv(url, row.names = 1)
x
```

> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

### Spotting major differences and trends 

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```


```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```


### Pairs plots and heatmaps

```{r}
pairs(x, col=rainbow(nrow(x)), pch=16)
```


```{r}
library(pheatmap)
pheatmap( as.matrix(x) )
```

## PCA to the rescue

The main PCA function in "base R" is called `prcomp()`. This function wants the transpose of our food data as input (i.e. the foods as columns and the countries as rows).

```{r}
pca <- prcomp(t(x))
summary(pca)
```

```{r}
attributes(pca)
```

To make one of main PCA result figures we turn to `pca$x` the scores along our new PCs. This is called "PC plot" or "score plot" or "Ordination plot" ...

```{r}
library(ggplot2)
my_cols <- c("orange", "red", "lightblue", "lightyellow")
ggplot(pca$x)+ aes(PC1,PC2, label= rownames(pca$x)) +
  geom_point(col= my_cols)
```


the second major result figure is called a "loadings plot" of "variable contributions plot" or "weight plot" 

```{r}
ggplot(pca$rotation) + 
  aes(PC1,rownames(pca$rotation)) + 
  geom_col()
```

