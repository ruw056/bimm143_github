---
title: "Class 8 Breast Cancer"
author: Eric Wang A17678188
format: pdf
toc: true
---

## Background

In today's class we will be employing all the R techniques for data analysis that we have learned thus far - including the machine learning methods of clustering and PCA - to analyze real breast cancer biopsy data. 

```{r}
fna.data <- "WisconsinCancer.csv"
wisc.df <-read.csv(fna.data, row.names=1)
```

Now we can view the data table. 

```{r}
head(wisc.df, 4)
```

Now we want to create a new data.frame that omits the first column

```{r}
wisc.data <- wisc.df[,-1]
diagnosis <- wisc.df[,1]
```

> Q1. How many observations are in this dataset?

```{r}
dim(wisc.data)
```
There are 569 observations in this data set. 

> Q2. How many observations have a malignant diagnosis?

```{r}
sum(diagnosis =="M")
```
There are 212 observations that have a malignant diagnosis. 

> Q3. How any variables/features in the data are suffixed with `_mean`?

```{r}
sum(grepl("_mean", colnames(wisc.df)))
```
There are 31 variables. 

## Performing PCA

Check the mean and standard deviation of the features of the `wisc.data` to determine if the data should be scaled

`scale=TRUE` is very important since the data are on very different scales in the OG data set. 
 
```{r}
colMeans(wisc.data)
apply(wisc.data,2,sd)
wisc.pr <- prcomp(wisc.data, scale= TRUE)
summary(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal component (PC1)

it is 0.4427
```{r}
prop.var <- summary(wisc.pr)$importance[3,]
prop.var
```

The proportion of variance would be 0.4427

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

```{r}
which(prop.var >= 0.7)[1]
```

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data? 

```{r}
which(prop.var >= 0.9)[1]
```



## Interpreting PCA results

```{r}
biplot(wisc.pr)
```

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

The plot is too messy and almost nothing can be identified.

```{r}
library(ggplot2)

ggplot(wisc.pr$x) + 
  aes(PC1, PC2, col = diagnosis) + 
  geom_point()
```

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
ggplot(wisc.pr$x) + aes(PC1, PC3, col = diagnosis) + geom_point()
```


## Variance explained

Calculate the variance of each principal component by squaring the sdev component of wisc.pr (i.e. wisc.pr$sdev^2). Save the result as an object called pr.var.

```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Calculate the variance explained by each principal component by dividing by the total variance explained of all principal components. Assign this to a variable called `pve` and create a plot of variance explained for each principal component.

```{r}
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(c(1,pve), xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```


```{r}
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```


## Communicating PCA results

```{r}
wisc.pr$rotation
```


> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC. Are there any features with larger contributions than this one?

```{r}
wisc.pr$rotation[,1]
wisc.pr$rotation["concave.points_mean",1]
```

```{r}
sort(abs(wisc.pr$rotation[, 1]), decreasing = TRUE)
```

There aren't any features that have larger contributions than this one. 





## Hierarchical clustering

The goal of this section is to do hierarchical clustering of the original data to see if there is any grouping into malignant and benign clusters. 


```{r}
data.scaled <- scale(wisc.data)

data.dist <- dist(data.scaled)

head(data.dist)

wisc.hclust <- hclust(data.dist, method = "complete")

plot(wisc.hclust)
```

```{r}
data.scaled <- scale(wisc.data)

data.dist <- dist(wisc.pr$x[,1:3])

head(data.dist)

wisc.pr.hclust <- hclust(data.dist, method = "ward.D2")

plot(wisc.pr.hclust)

```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=2)

wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

> Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.pr.hclust)
abline(h = 19, col="red", lty=2)
```



> Q12. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.


It would also be `ward.D2` because it gave much better visual than other methods. 



## Combing methods


```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

```{r}
ggplot(wisc.pr$x) +
  aes(PC1, PC2) +
  geom_point(col=grps)
```

> Q.13. How well does the newly created hclust model with two clusters separate out the two "M" and "B" diagnoses? 

```{r}
table(wisc.pr.hclust.clusters, diagnosis)
```

> Q14. How well do the hierarchical clustering models you created in the previous sections (i.e. without first doing PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.hclust.clusters and wisc.pr.hclust.clusters) with the vector containing the actual diagnoses.

```{r}
table(wisc.hclust.clusters, diagnosis)
```



## Prediction

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

> Q16. Which of these new patients should we prioritize for follow up based on your results?

We should prioritize patient 1 first because they are at a higher risk since they look more similar to the malignant cases